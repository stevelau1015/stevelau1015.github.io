<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>机器学习：CS229  | leon21</title>
<link rel="shortcut icon" href="https://stevelau1015.github.io/favicon.ico?v=1631297957633">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://stevelau1015.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="机器学习：CS229  | leon21 - Atom Feed" href="https://stevelau1015.github.io/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-194058776-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-194058776-1');
</script>


    <meta name="description" content="CS229 学习笔记
梯度下降应该同步更新
缺少图片
lr 设置要合理

评价函数里对所有样本做了遍历，于是为BGD Batch

奇异矩阵没有逆矩阵

特征缩放

平均标准化
画这个图像来确认梯度下降是否收敛

设置合适的学习率，不然梯度..." />
    <meta name="keywords" content="Machine Learning" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://stevelau1015.github.io">
  <img class="avatar" src="https://stevelau1015.github.io/images/avatar.png?v=1631297957633" alt="">
  </a>
  <h1 class="site-title">
    leon21
  </h1>
  <p class="site-description">
    温故而知新
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          档案
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/stevelau1015/" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
    
      
        <a href="https://weibo.com/stevelau1015/" target="_blank">
          <i class="ri-weibo-line"></i>
        </a>
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              机器学习：CS229 
            </h2>
            <div class="post-info">
              <span>
                2021-01-08
              </span>
              <span>
                4 min read
              </span>
              
                <a href="https://stevelau1015.github.io/tag/ZQvoL-G_Q/" class="post-tag">
                  # Machine Learning
                </a>
              
            </div>
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <h1 id="cs229-学习笔记">CS229 学习笔记</h1>
<h2 id="梯度下降应该同步更新">梯度下降应该同步更新</h2>
<p>缺少图片</p>
<h2 id="lr-设置要合理">lr 设置要合理</h2>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20210108163016212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="" loading="lazy"></figure>
<h3 id="评价函数里对所有样本做了遍历于是为bgd-batch">评价函数里对所有样本做了遍历，于是为BGD Batch</h3>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20210108163209784.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt=" batch" loading="lazy"></figure>
<h1 id="奇异矩阵没有逆矩阵">奇异矩阵没有逆矩阵</h1>
<figure data-type="image" tabindex="3"><img src="https://img-blog.csdnimg.cn/20210108163253317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="特征缩放">特征缩放</h2>
<figure data-type="image" tabindex="4"><img src="https://img-blog.csdnimg.cn/20210108163332197.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="平均标准化">平均标准化<img src="https://img-blog.csdnimg.cn/20210108163347730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></h3>
<h2 id="画这个图像来确认梯度下降是否收敛">画这个图像来确认梯度下降是否收敛</h2>
<figure data-type="image" tabindex="5"><img src="https://img-blog.csdnimg.cn/20210108163528164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="画这个图像来确认梯度下降是否收敛" loading="lazy"></figure>
<h3 id="设置合适的学习率不然梯度下降效率会很慢">设置合适的学习率，不然梯度下降效率会很慢</h3>
<figure data-type="image" tabindex="6"><img src="https://img-blog.csdnimg.cn/20210108163701302.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="合适的学习率，不然梯度下降效率会很慢" loading="lazy"></figure>
<h2 id="octave教学">Octave教学</h2>
<figure data-type="image" tabindex="7"><img src="https://img-blog.csdnimg.cn/20210108163732959.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="高斯分布" loading="lazy"></figure>
<h2 id="normal-equation-正规方程">Normal Equation 正规方程</h2>
<p>每行1个样本 包含了样本1的好多个feature<br>
每个点是一个feature的值<br>
<img src="https://img-blog.csdnimg.cn/20210108174838623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="X矩阵 设计矩阵 每行1个样本 每个点是一个feature的值" loading="lazy"></p>
<h2 id="用normal-equation时缩放是没有必要的">用normal equation时，缩放是没有必要的</h2>
<figure data-type="image" tabindex="8"><img src="https://img-blog.csdnimg.cn/20210108175558775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="normal equation 左上" loading="lazy"></figure>
<h1 id="logistic-regression-新cost-函数来自于极大似然估计">logistic regression 新cost 函数来自于极大似然估计</h1>
<figure data-type="image" tabindex="9"><img src="https://img-blog.csdnimg.cn/20210109093625358.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="极大似然估计" loading="lazy"></figure>
<h3 id="和linear-regression-的区别在于hx的假设不一样了">和linear regression 的区别在于h(x)的假设不一样了</h3>
<p><img src="https://img-blog.csdnimg.cn/20210109094042619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
相当于把非线性的sigmoid函数塞进了h（x）里<br>
线搜索使得不需要手动调整α<br>
<img src="https://img-blog.csdnimg.cn/20210109094622667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h3 id="多分类问题">多分类问题</h3>
<p>输入相同只需要把三个分类器里算出的值比较一下，找出值最大的那一个分类器，那么将被分入这个分类器对应的那一类<br>
<img src="https://img-blog.csdnimg.cn/20210109100754795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="overfitting-high-variance-过拟合高方差">overfitting  High variance 过拟合=高方差</h2>
<p>泛化能力，模型应用到新样本的能力<br>
<img src="https://img-blog.csdnimg.cn/2021010910141780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
如何识别overfitting 后期会学习<br>
如何解决overfitting<br>
两种方法 1：减少feature数量<br>
2：正则化 regularization（每个feature贡献一部分模型）		<img src="https://img-blog.csdnimg.cn/20210109101929124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
正则化：加了一些惩罚项，目标是最小化等号右边的式子，只有θ3和θ4接近于零的时候才让等号右边的式子最小<br>
<img src="https://img-blog.csdnimg.cn/20210109102312926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
正则化必定可以让normal equation 的前面这一项变得可逆<br>
<img src="https://img-blog.csdnimg.cn/20210109103716604.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
对所有正则化来说，lamda是用来取舍fit the trainset 和 generalization的一种办法，一种比重，一种比例</p>
<h1 id="神经网络">神经网络</h1>
<p><img src="https://img-blog.csdnimg.cn/20210109111207288.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="数学假设" loading="lazy"><br>
右上角是层数，线性，sigmoid，再线性，再sigmoid，输出</p>
<h2 id="cost-function-lesson-9-1">cost function （lesson 9-1）</h2>
<p>一大坨和<br>
<img src="https://img-blog.csdnimg.cn/20210109173830711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="后向传播-lesson-9-2">后向传播 （lesson 9-2）</h2>
<p>左下到右下的数学推导欠缺。<br>
但是可以利用a和δ求出偏导<br>
<img src="https://img-blog.csdnimg.cn/20210109173523735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h3 id="具体实现的流程">具体实现的流程</h3>
<p>不需要δ（1）和δ（L） ：输入层和输出层没有误差<br>
<img src="https://img-blog.csdnimg.cn/20210109174241730.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
j=0，对应着偏差项，无需正则化<br>
<img src="https://img-blog.csdnimg.cn/20210109174538200.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
最后算出来一个D值，就是损失函数里的偏导项，可以在这个基础上继续梯度下降<img src="https://img-blog.csdnimg.cn/20210109174734539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="零初始化的重要性全部一样的初始权重将导致一模一样的输入结果所有网络节点都一样计算没有意义">零初始化的重要性，全部一样的初始权重将导致一模一样的输入结果，所有网络节点都一样，计算没有意义</h2>
<h1 id="建议的策略-不要过早优化">建议的策略 不要过早优化</h1>
<figure data-type="image" tabindex="10"><img src="https://img-blog.csdnimg.cn/20210110181048782.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h1 id="误差分析">误差分析</h1>
<figure data-type="image" tabindex="11"><img src="https://img-blog.csdnimg.cn/20210110181519866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h1 id="不均衡的分类问题-skewed">不均衡的分类问题 skewed</h1>
<p>把较少的那一类设置为y=1 ，阳性类，由此得出precision和recall很难欺骗我们。<br>
<img src="https://img-blog.csdnimg.cn/20210110183607503.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="precision和recall的权衡">precision和recall的权衡</h2>
<p>高 precision 设置阈值（threshold）为0.7<br>
高 recall 设置阈值为0.3<br>
<img src="https://img-blog.csdnimg.cn/20210110183359334.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
用一个<em>积在河上飞</em>的F1score去评价precision和recall的权衡<br>
<img src="https://img-blog.csdnimg.cn/20210111144710173.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h1 id="是否需要更大的数据集">是否需要更大的数据集</h1>
<p>当人类专家能从给定信息里判断，那么不需要<br>
<img src="https://img-blog.csdnimg.cn/20210110185218257.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
更大的数据集通常用来使系统 low variance<br>
复杂的算法使系统 low bias<br>
<img src="https://img-blog.csdnimg.cn/20210110185512263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h1 id="12-1-svm">12-1 Svm</h1>
<p>SVM和LR的区别：损失函数做了替换，正则项的系数更考虑贴合前一项，也就是更考虑和样本的符合程度。<br>
<img src="https://img-blog.csdnimg.cn/20210215112322391.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="12-2-svm-c不太大的时候会合理忽略异常大间隔导致了鲁棒性用尽量大的间隔把样本分开">12-2 svm C不太大的时候会合理忽略异常，大间隔导致了鲁棒性，用尽量大的间隔把样本分开</h2>
<h2 id="12-3-θ_0-0边界过原点-时候的分类效果数学推导">12-3 θ_0 = 0(边界过原点） 时候的分类效果，数学推导</h2>
<p>设置边界如左将使得正负两样本投影到θ上的向量变小，这样使得为满足s.t.的话，θ的模的平方，势必要很大，不符合第一行minimal 的假设。所以能看出SVM是大间隔的分类器，为了让p_1,p_2.p_3（间隔margin）更大。<br>
<img src="https://img-blog.csdnimg.cn/20210215120106497.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h1 id="跳过了12-1~13-5的svm和k-means聚类待补充共11个视频">跳过了12-1~13-5的SVM和K-means聚类（待补充）共11个视频</h1>
<h1 id="降维pca-主成分分析">降维：PCA 主成分分析</h1>
<p>formula：目的是找到一个向量，使得所有点投影到这个方向上延伸的直线的投影距离最短<br>
<img src="https://img-blog.csdnimg.cn/2021011114492386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
pca不是linear regression<br>
计算误差的方式不一样，linear是算竖直距离；PCA求的是最短距离<br>
<img src="https://img-blog.csdnimg.cn/20210111145246450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
有字幕版本的14-5 https://www.bilibili.com/video/av95751735</p>
<h2 id="如何查看合适的维度选择降到几维呢">如何查看合适的维度，选择降到几维呢？</h2>
<p>用svd解出的S值，就可以算出协方差，covariance是否小于0.05/0.01，得到合理的K值<br>
<img src="https://img-blog.csdnimg.cn/20210111151925385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
<img src="https://img-blog.csdnimg.cn/20210111152405951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="恢复压缩">恢复压缩</h2>
<p>两边左乘一个svd中的U<br>
<img src="https://img-blog.csdnimg.cn/20210111153010877.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4ODM1NA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li><a href="#cs229-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0">CS229 学习笔记</a>
<ul>
<li><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%BA%94%E8%AF%A5%E5%90%8C%E6%AD%A5%E6%9B%B4%E6%96%B0">梯度下降应该同步更新</a></li>
<li><a href="#lr-%E8%AE%BE%E7%BD%AE%E8%A6%81%E5%90%88%E7%90%86">lr 设置要合理</a>
<ul>
<li><a href="#%E8%AF%84%E4%BB%B7%E5%87%BD%E6%95%B0%E9%87%8C%E5%AF%B9%E6%89%80%E6%9C%89%E6%A0%B7%E6%9C%AC%E5%81%9A%E4%BA%86%E9%81%8D%E5%8E%86%E4%BA%8E%E6%98%AF%E4%B8%BAbgd-batch">评价函数里对所有样本做了遍历，于是为BGD Batch</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E5%A5%87%E5%BC%82%E7%9F%A9%E9%98%B5%E6%B2%A1%E6%9C%89%E9%80%86%E7%9F%A9%E9%98%B5">奇异矩阵没有逆矩阵</a>
<ul>
<li><a href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE">特征缩放</a>
<ul>
<li><a href="#%E5%B9%B3%E5%9D%87%E6%A0%87%E5%87%86%E5%8C%96">平均标准化!在这里插入图片描述</a></li>
</ul>
</li>
<li><a href="#%E7%94%BB%E8%BF%99%E4%B8%AA%E5%9B%BE%E5%83%8F%E6%9D%A5%E7%A1%AE%E8%AE%A4%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%98%AF%E5%90%A6%E6%94%B6%E6%95%9B">画这个图像来确认梯度下降是否收敛</a>
<ul>
<li><a href="#%E8%AE%BE%E7%BD%AE%E5%90%88%E9%80%82%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%B8%8D%E7%84%B6%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%95%88%E7%8E%87%E4%BC%9A%E5%BE%88%E6%85%A2">设置合适的学习率，不然梯度下降效率会很慢</a></li>
</ul>
</li>
<li><a href="#octave%E6%95%99%E5%AD%A6">Octave教学</a></li>
<li><a href="#normal-equation-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B">Normal Equation 正规方程</a></li>
<li><a href="#%E7%94%A8normal-equation%E6%97%B6%E7%BC%A9%E6%94%BE%E6%98%AF%E6%B2%A1%E6%9C%89%E5%BF%85%E8%A6%81%E7%9A%84">用normal equation时，缩放是没有必要的</a></li>
</ul>
</li>
<li><a href="#logistic-regression-%E6%96%B0cost-%E5%87%BD%E6%95%B0%E6%9D%A5%E8%87%AA%E4%BA%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1">logistic regression 新cost 函数来自于极大似然估计</a><br>
*
<ul>
<li><a href="#%E5%92%8Clinear-regression-%E7%9A%84%E5%8C%BA%E5%88%AB%E5%9C%A8%E4%BA%8Ehx%E7%9A%84%E5%81%87%E8%AE%BE%E4%B8%8D%E4%B8%80%E6%A0%B7%E4%BA%86">和linear regression 的区别在于h(x)的假设不一样了</a></li>
<li><a href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98">多分类问题</a></li>
<li><a href="#overfitting-high-variance-%E8%BF%87%E6%8B%9F%E5%90%88%E9%AB%98%E6%96%B9%E5%B7%AE">overfitting  High variance 过拟合=高方差</a></li>
</ul>
</li>
<li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>
<ul>
<li><a href="#cost-function-lesson-9-1">cost function （lesson 9-1）</a></li>
<li><a href="#%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD-lesson-9-2">后向传播 （lesson 9-2）</a>
<ul>
<li><a href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%B5%81%E7%A8%8B">具体实现的流程</a></li>
</ul>
</li>
<li><a href="#%E9%9B%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7%E5%85%A8%E9%83%A8%E4%B8%80%E6%A0%B7%E7%9A%84%E5%88%9D%E5%A7%8B%E6%9D%83%E9%87%8D%E5%B0%86%E5%AF%BC%E8%87%B4%E4%B8%80%E6%A8%A1%E4%B8%80%E6%A0%B7%E7%9A%84%E8%BE%93%E5%85%A5%E7%BB%93%E6%9E%9C%E6%89%80%E6%9C%89%E7%BD%91%E7%BB%9C%E8%8A%82%E7%82%B9%E9%83%BD%E4%B8%80%E6%A0%B7%E8%AE%A1%E7%AE%97%E6%B2%A1%E6%9C%89%E6%84%8F%E4%B9%89">零初始化的重要性，全部一样的初始权重将导致一模一样的输入结果，所有网络节点都一样，计算没有意义</a></li>
</ul>
</li>
<li><a href="#%E5%BB%BA%E8%AE%AE%E7%9A%84%E7%AD%96%E7%95%A5-%E4%B8%8D%E8%A6%81%E8%BF%87%E6%97%A9%E4%BC%98%E5%8C%96">建议的策略 不要过早优化</a></li>
<li><a href="#%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90">误差分析</a></li>
<li><a href="#%E4%B8%8D%E5%9D%87%E8%A1%A1%E7%9A%84%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98-skewed">不均衡的分类问题 skewed</a>
<ul>
<li><a href="#precision%E5%92%8Crecall%E7%9A%84%E6%9D%83%E8%A1%A1">precision和recall的权衡</a></li>
</ul>
</li>
<li><a href="#%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E6%9B%B4%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86">是否需要更大的数据集</a></li>
<li><a href="#12-1-svm">12-1 Svm</a>
<ul>
<li><a href="#12-2-svm-c%E4%B8%8D%E5%A4%AA%E5%A4%A7%E7%9A%84%E6%97%B6%E5%80%99%E4%BC%9A%E5%90%88%E7%90%86%E5%BF%BD%E7%95%A5%E5%BC%82%E5%B8%B8%E5%A4%A7%E9%97%B4%E9%9A%94%E5%AF%BC%E8%87%B4%E4%BA%86%E9%B2%81%E6%A3%92%E6%80%A7%E7%94%A8%E5%B0%BD%E9%87%8F%E5%A4%A7%E7%9A%84%E9%97%B4%E9%9A%94%E6%8A%8A%E6%A0%B7%E6%9C%AC%E5%88%86%E5%BC%80">12-2 svm C不太大的时候会合理忽略异常，大间隔导致了鲁棒性，用尽量大的间隔把样本分开</a></li>
<li><a href="#12-3-%CE%B8_0-0%E8%BE%B9%E7%95%8C%E8%BF%87%E5%8E%9F%E7%82%B9-%E6%97%B6%E5%80%99%E7%9A%84%E5%88%86%E7%B1%BB%E6%95%88%E6%9E%9C%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC">12-3 θ_0 = 0(边界过原点） 时候的分类效果，数学推导</a></li>
</ul>
</li>
<li><a href="#%E8%B7%B3%E8%BF%87%E4%BA%8612-1~13-5%E7%9A%84svm%E5%92%8Ck-means%E8%81%9A%E7%B1%BB%E5%BE%85%E8%A1%A5%E5%85%85%E5%85%B111%E4%B8%AA%E8%A7%86%E9%A2%91">跳过了12-1~13-5的SVM和K-means聚类（待补充）共11个视频</a></li>
<li><a href="#%E9%99%8D%E7%BB%B4pca-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">降维：PCA 主成分分析</a>
<ul>
<li><a href="#%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E5%90%88%E9%80%82%E7%9A%84%E7%BB%B4%E5%BA%A6%E9%80%89%E6%8B%A9%E9%99%8D%E5%88%B0%E5%87%A0%E7%BB%B4%E5%91%A2">如何查看合适的维度，选择降到几维呢？</a></li>
<li><a href="#%E6%81%A2%E5%A4%8D%E5%8E%8B%E7%BC%A9">恢复压缩</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/stevelau1015/" target="_blank"> Leon21 </a>
  <a class="rss" href="https://stevelau1015.github.io/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
